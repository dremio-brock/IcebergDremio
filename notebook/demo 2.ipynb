{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490e93d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-binance\n",
      "  Downloading python_binance-1.0.15-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pyspark in /usr/local/spark-3.1.2-bin-hadoop3.2/python (3.1.2)\n",
      "Collecting websockets==9.1\n",
      "  Downloading websockets-9.1-cp39-cp39-manylinux2010_x86_64.whl (102 kB)\n",
      "\u001b[K     |████████████████████████████████| 102 kB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dateparser\n",
      "  Downloading dateparser-1.1.0-py2.py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 41.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from python-binance) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from python-binance) (2.26.0)\n",
      "Collecting ujson\n",
      "  Downloading ujson-4.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
      "\u001b[K     |████████████████████████████████| 218 kB 32.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.9/site-packages (from pyspark) (0.10.9)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (203 kB)\n",
      "\u001b[K     |████████████████████████████████| 203 kB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->python-binance) (21.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n",
      "\u001b[K     |████████████████████████████████| 304 kB 34.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->python-binance) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->python-binance) (3.10.0.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp->python-binance) (3.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from dateparser->python-binance) (2.8.2)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-4.1-py3-none-any.whl (19 kB)\n",
      "Collecting regex!=2019.02.19,!=2021.8.27\n",
      "  Downloading regex-2021.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "\u001b[K     |████████████████████████████████| 763 kB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from dateparser->python-binance) (2021.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->python-binance) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->python-binance) (1.26.6)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2021.5-py2.py3-none-any.whl (339 kB)\n",
      "\u001b[K     |████████████████████████████████| 339 kB 44.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tzdata, pytz-deprecation-shim, multidict, frozenlist, yarl, tzlocal, regex, async-timeout, aiosignal, websockets, ujson, dateparser, aiohttp, python-binance\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 dateparser-1.1.0 frozenlist-1.2.0 multidict-5.2.0 python-binance-1.0.15 pytz-deprecation-shim-0.1.0.post0 regex-2021.11.10 tzdata-2021.5 tzlocal-4.1 ujson-4.2.0 websockets-9.1 yarl-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-binance pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6101fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "import configparser\n",
    "import iceberg_spark\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "spark, jvm, hive_uri, sc = iceberg_spark.start_spark('s3', 'aws', 'hadoop')\n",
    "\n",
    "# Read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "api_secret = config['binance']['secret']\n",
    "api_key = config['binance']['key']\n",
    "\n",
    "# create client\n",
    "client = Client(api_key, api_secret, tld='us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28347e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 04:17:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "21/11/18 04:17:35 WARN HadoopTableOperations: Error reading version hint file s3a://brock-dremio/iceberg/warehouse/default/tickers/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://brock-dremio/iceberg/warehouse/default/tickers/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2269)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:702)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:316)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:99)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$2(BaseTransaction.java:290)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:405)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:214)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:198)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:190)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:287)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:233)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:112)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:465)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:178)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.$anonfun$runCommand$1(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:133)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/18 04:17:36 WARN HadoopTableOperations: Error reading version hint file s3a://brock-dremio/iceberg/warehouse/default/tickers/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://brock-dremio/iceberg/warehouse/default/tickers/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2269)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:702)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:316)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:99)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:80)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$2(BaseTransaction.java:299)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:405)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:214)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:198)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:190)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:287)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:233)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:112)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:35)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:465)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:178)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.$anonfun$runCommand$1(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:133)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# get current prices from binance\n",
    "tickers = client.get_all_tickers()\n",
    "\n",
    "#convert to a spark dataframe, cast data type to decimal, and add date and timestamp columns\n",
    "tickersDF = spark.createDataFrame(data=tickers)\n",
    "tickersDF = tickersDF.withColumn(\"price\", col(\"price\").cast(\"decimal(38,8)\"))\n",
    "tickersDF = tickersDF.withColumn(\"date\", current_date()).withColumn(\"ts\", current_timestamp())\n",
    "\n",
    "\n",
    "#write to an iceberg table\n",
    "tickersDF.writeTo('tickers').using('iceberg').partitionedBy('date').createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bc036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------+--------------------+\n",
      "|         price| symbol|      date|                  ts|\n",
      "+--------------+-------+----------+--------------------+\n",
      "|59910.49000000| BTCUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "| 4286.50000000| ETHUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "|    0.29700000| XRPUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "|  589.18000000| BCHUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "|  222.25000000| LTCUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "|    1.00060000|USDTUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "|59871.73000000|BTCUSDT|2021-11-18|2021-11-18 04:17:...|\n",
      "| 4284.68000000|ETHUSDT|2021-11-18|2021-11-18 04:17:...|\n",
      "|    0.29691000|XRPUSDT|2021-11-18|2021-11-18 04:17:...|\n",
      "|  588.56000000|BCHUSDT|2021-11-18|2021-11-18 04:17:...|\n",
      "|  221.96000000|LTCUSDT|2021-11-18|2021-11-18 04:17:...|\n",
      "|  580.19280000| BNBUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "|  579.72350000|BNBUSDT|2021-11-18|2021-11-18 04:17:...|\n",
      "|    0.07153000| ETHBTC|2021-11-18|2021-11-18 04:17:...|\n",
      "|    0.00000864| XRPBTC|2021-11-18|2021-11-18 04:17:...|\n",
      "|    0.00968340| BNBBTC|2021-11-18|2021-11-18 04:17:...|\n",
      "|    0.00370800| LTCBTC|2021-11-18|2021-11-18 04:17:...|\n",
      "|    0.00983200| BCHBTC|2021-11-18|2021-11-18 04:17:...|\n",
      "|    1.88100000| ADAUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "|    1.06260000| BATUSD|2021-11-18|2021-11-18 04:17:...|\n",
      "+--------------+-------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.table(\"tickers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48aff613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create funtion to append data\n",
    "def get_tickers():\n",
    "    # get current prices from binance\n",
    "    tickers = client.get_all_tickers()\n",
    "    \n",
    "    #convert to a spark dataframe & add date and timestamp columns\n",
    "    tickersDF = spark.createDataFrame(data=tickers)\n",
    "    tickersDF = tickersDF.withColumn(\"price\", col(\"price\").cast(\"decimal(38,8)\"))\n",
    "    tickersDF = tickersDF.withColumn(\"date\", current_date()).withColumn(\"ts\", current_timestamp())\n",
    "    \n",
    "    \n",
    "    #write to an iceberg table\n",
    "    tickersDF.writeTo('tickers').append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed81aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to repeat a task\n",
    "def load_tickers(interval):\n",
    "    while True:\n",
    "        get_tickers()\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4eb77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3195/4204878840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# call periodic work every 10 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mload_tickers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3195/197514257.py\u001b[0m in \u001b[0;36mload_tickers\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mget_tickers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# call periodic work every 10 seconds\n",
    "load_tickers(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dbcf0",
   "metadata": {},
   "source": [
    "# Spark Writes\n",
    "(https://iceberg.apache.org/spark-writes/) [https://iceberg.apache.org/spark-writes/]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ee0ae",
   "metadata": {},
   "source": [
    "# Writing with DataFrames\n",
    "[apache Iceberg](https://iceberg.apache.org/spark-writes/#writing-with-dataframes)\n",
    "<ul>\n",
    "    <li>df.writeTo(t).create() is equivalent to CREATE TABLE AS SELECT</li>\n",
    "    <li>df.writeTo(t).replace() is equivalent to REPLACE TABLE AS SELECT</li>\n",
    "    <li>df.writeTo(t).append() is equivalent to INSERT INTO</li>\n",
    "    <li>df.writeTo(t).overwritePartitions() is equivalent to dynamic INSERT OVERWRITE</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f5ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
